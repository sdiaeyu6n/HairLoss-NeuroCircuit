{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6X4-SP7vq4Ll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms # 이미지 데이터 transform\n",
        "from torch.utils.data import DataLoader # 이미지 데이터 로더\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#코랩 환경\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "0tuNVz9zq9rG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "라벨링 데이터 로드"
      ],
      "metadata": {
        "id": "C-Ei7ytiMfsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴로서킷 라벨 엑셀 데이터 로드\n",
        "# train 데이터 600개 + test 데이터 81개\n",
        "df = pd.read_csv('drive/MyDrive/기사프/neurocircuit_label_data.csv')\n",
        "\n",
        "# 두피 타입 O, C"
      ],
      "metadata": {
        "id": "8520-NgwMfSb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 예제코드\n",
        "# dolphin_dir = '/dolphin/'\n",
        "# shark_dir = '/shark/'\n",
        "# whale_dir = '/whale/'"
      ],
      "metadata": {
        "id": "v_RcrKYRrAxA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지 데이터 전처리\n",
        "파이토치 라이브러리, albumentation 라이브러리 이용\n",
        "데이터 커스터마이징 2가지 방식으로 진행\n",
        "\n",
        "1. 이미지 resize\n",
        "2. 스케일링을 준비\n",
        "3. augmentation transform\n",
        "4. data loader로 학습 진행"
      ],
      "metadata": {
        "id": "v78LnEUdrFdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalpDataSet(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): csv 파일의 경로\n",
        "            root_dir (string): 모든 이미지가 존재하는 디렉토리 경로\n",
        "            transform (callable, optional): 샘플에 적용될 Optional transform\n",
        "        \"\"\"\n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "i9xOyIy-3K4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "resize_trans = transforms.Compose([\n",
        "                                   transforms.Resize((128,128)),\n",
        "                                   transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#데이터 불러오기\n",
        "resize_data = torchvision.datasets.ImageFolder(root=df, transform=resize_trans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "VvFFqXlJg9xi",
        "outputId": "58e1016b-7d88-47c4-adca-74efc3a7eafd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b74f22664096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#데이터 불러오기 어케 할건지...?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresize_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     ) -> None:\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: scandir: path should be string, bytes, os.PathLike, integer or None, not DataFrame"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resize_train[0][0].shape # ToTensor를 진행했기 때문에 데이터가 torch(C,H,W) 형태로 바뀜"
      ],
      "metadata": {
        "id": "ywL9zsFzjXBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy로 바꾸고, axis = 1,2 mean으로 RGB mean/std 뽑기\n",
        "np.mean(resize_train[0][0].numpy(),axis=(1,2)) "
      ],
      "metadata": {
        "id": "Itje5PzjncGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(dataset):\n",
        "  meanRGB = [np.mean(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
        "  stdRGB = [np.std(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
        "\n",
        "  meanR = np.mean([m[0] for m in meanRGB])\n",
        "  meanG = np.mean([m[1] for m in meanRGB])\n",
        "  meanB = np.mean([m[2] for m in meanRGB])\n",
        "\n",
        "  stdR = np.mean([s[0] for s in stdRGB])\n",
        "  stdG = np.mean([s[1] for s in stdRGB])\n",
        "  stdB = np.mean([s[2] for s in stdRGB])\n",
        "\n",
        "  print(meanR, meanG, meanB)\n",
        "  print(stdR, stdG, stdB)"
      ],
      "metadata": {
        "id": "7hJGJAfGntwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalization 준비\n",
        "\n",
        "resize_train_mean=[0.17191947, 0.41128376, 0.56153077]\n",
        "resize_train_std=[0.16150557, 0.16577946, 0.16063999]\n",
        "\n",
        "resize_test_mean=[0.15918699, 0.410329, 0.55247366]\n",
        "resize_test_std=[0.1542138, 0.16098696, 0.15552239]"
      ],
      "metadata": {
        "id": "ys6R4sYznzrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. transform을 이용해 data augmentation하기"
      ],
      "metadata": {
        "id": "v9YqUrhMn6_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((128, 128)), # 이미지 resize\n",
        "    transforms.RandomCrop(124), # 이미지를 랜덤으로 크롭\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), # 이미지 지터링(밝기, 대조, 채비, 색조)\n",
        "    transforms.RandomHorizontalFlip(p = 1), # p확률로 이미지 좌우반전\n",
        "    transforms.RandomVerticalFlip(p = 1), # p확률로 상하반전\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(resize_train_mean, resize_train_std)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((128, 128)), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(resize_test_mean, resize_test_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root=train_path, transform=transform_train)\n",
        "testset = torchvision.datasets.ImageFolder(root=test_path, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
        "                                         shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "OpZYXE3cn8aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images.shape"
      ],
      "metadata": {
        "id": "OUY3LNPYoBir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "augmentation을 적용한 이미지 확인하기"
      ],
      "metadata": {
        "id": "qapLS1IFoGHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def imshow(img, mean, std):\n",
        "    npimg = img.numpy()\n",
        "    img = np.transpose( npimg, (1, 2, 0) )\n",
        "    img = img * std + mean # renormalize\n",
        "    img = img.clip(0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "none_trans = torchvision.datasets.ImageFolder(root=train_path, transform=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "sJipwTNEoCBl",
        "outputId": "ee92fb79-5f63-4166-fe7f-b91d1b012da6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8ccae3bfe2af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnone_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 이미지\n",
        "none_trans[359][0]"
      ],
      "metadata": {
        "id": "Lhr3NUWNoNSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k9_gs_I7oNLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 준비 End\n",
        "Resnet 아키텍쳐 구성 부분"
      ],
      "metadata": {
        "id": "IPmXJApBq0AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import resnet\n",
        "import torchvision.models.resnet as resnet\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 미리 정의\n",
        "conv1x1=resnet.conv1x1\n",
        "Bottleneck = resnet.Bottleneck\n",
        "BasicBlock= resnet.BasicBlock"
      ],
      "metadata": {
        "id": "Sl1s0Fblq2JB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 32 # conv1에서 나올 채널의 차원 -> 이미지넷보다 작은 데이터이므로 32로 조정\n",
        "\n",
        "        # inputs = 3x224x224 -> 3x128x128로 바뀜\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False) # 마찬가지로 전부 사이즈 조정\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1) # 3 반복\n",
        "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2) # 4 반복\n",
        "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2) # 6 반복\n",
        "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2) # 3 반복\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1): # planes -> 입력되는 채널 수\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion: \n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input [32, 128, 128] -> [C ,H, W]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        #x.shape =[32, 64, 64]\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        #x.shape =[128, 64, 64]\n",
        "        x = self.layer2(x)\n",
        "        #x.shape =[256, 32, 32]\n",
        "        x = self.layer3(x)\n",
        "        #x.shape =[512, 16, 16]\n",
        "        x = self.layer4(x)\n",
        "        #x.shape =[1024, 8, 8]\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "TJwZ4DVnq4Qz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 3, True).to(device) \n",
        "# resnet50\n",
        "# 1(conv1) + 9(layer1) + 12(layer2) + 18(layer3) + 9(layer4) +1(fc)= ResNet50"
      ],
      "metadata": {
        "id": "5PqOBoGSq6hY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "구성한 아키텍쳐 확인"
      ],
      "metadata": {
        "id": "fw5LuXcaq-_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력 tensor가 맞는지 확인해보자\n",
        "from torchsummary import summary\n",
        "summary(resnet50, input_size=(3, 128, 128), device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cCnR3aSrBHq",
        "outputId": "242ace3d-8f0c-41ad-e8b7-7d320ac7c7ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 128, 128]             864\n",
            "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
            "              ReLU-3         [-1, 32, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 32, 64, 64]               0\n",
            "            Conv2d-5           [-1, 32, 64, 64]           1,024\n",
            "       BatchNorm2d-6           [-1, 32, 64, 64]              64\n",
            "              ReLU-7           [-1, 32, 64, 64]               0\n",
            "            Conv2d-8           [-1, 32, 64, 64]           9,216\n",
            "       BatchNorm2d-9           [-1, 32, 64, 64]              64\n",
            "             ReLU-10           [-1, 32, 64, 64]               0\n",
            "           Conv2d-11          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
            "           Conv2d-13          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-14          [-1, 128, 64, 64]             256\n",
            "             ReLU-15          [-1, 128, 64, 64]               0\n",
            "       Bottleneck-16          [-1, 128, 64, 64]               0\n",
            "           Conv2d-17           [-1, 32, 64, 64]           4,096\n",
            "      BatchNorm2d-18           [-1, 32, 64, 64]              64\n",
            "             ReLU-19           [-1, 32, 64, 64]               0\n",
            "           Conv2d-20           [-1, 32, 64, 64]           9,216\n",
            "      BatchNorm2d-21           [-1, 32, 64, 64]              64\n",
            "             ReLU-22           [-1, 32, 64, 64]               0\n",
            "           Conv2d-23          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
            "             ReLU-25          [-1, 128, 64, 64]               0\n",
            "       Bottleneck-26          [-1, 128, 64, 64]               0\n",
            "           Conv2d-27           [-1, 32, 64, 64]           4,096\n",
            "      BatchNorm2d-28           [-1, 32, 64, 64]              64\n",
            "             ReLU-29           [-1, 32, 64, 64]               0\n",
            "           Conv2d-30           [-1, 32, 64, 64]           9,216\n",
            "      BatchNorm2d-31           [-1, 32, 64, 64]              64\n",
            "             ReLU-32           [-1, 32, 64, 64]               0\n",
            "           Conv2d-33          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
            "             ReLU-35          [-1, 128, 64, 64]               0\n",
            "       Bottleneck-36          [-1, 128, 64, 64]               0\n",
            "           Conv2d-37           [-1, 64, 64, 64]           8,192\n",
            "      BatchNorm2d-38           [-1, 64, 64, 64]             128\n",
            "             ReLU-39           [-1, 64, 64, 64]               0\n",
            "           Conv2d-40           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-41           [-1, 64, 32, 32]             128\n",
            "             ReLU-42           [-1, 64, 32, 32]               0\n",
            "           Conv2d-43          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
            "           Conv2d-45          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-46          [-1, 256, 32, 32]             512\n",
            "             ReLU-47          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-48          [-1, 256, 32, 32]               0\n",
            "           Conv2d-49           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-50           [-1, 64, 32, 32]             128\n",
            "             ReLU-51           [-1, 64, 32, 32]               0\n",
            "           Conv2d-52           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-53           [-1, 64, 32, 32]             128\n",
            "             ReLU-54           [-1, 64, 32, 32]               0\n",
            "           Conv2d-55          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-56          [-1, 256, 32, 32]             512\n",
            "             ReLU-57          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-58          [-1, 256, 32, 32]               0\n",
            "           Conv2d-59           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-60           [-1, 64, 32, 32]             128\n",
            "             ReLU-61           [-1, 64, 32, 32]               0\n",
            "           Conv2d-62           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
            "             ReLU-64           [-1, 64, 32, 32]               0\n",
            "           Conv2d-65          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-66          [-1, 256, 32, 32]             512\n",
            "             ReLU-67          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-68          [-1, 256, 32, 32]               0\n",
            "           Conv2d-69           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-70           [-1, 64, 32, 32]             128\n",
            "             ReLU-71           [-1, 64, 32, 32]               0\n",
            "           Conv2d-72           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-73           [-1, 64, 32, 32]             128\n",
            "             ReLU-74           [-1, 64, 32, 32]               0\n",
            "           Conv2d-75          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
            "             ReLU-77          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-78          [-1, 256, 32, 32]               0\n",
            "           Conv2d-79          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-80          [-1, 128, 32, 32]             256\n",
            "             ReLU-81          [-1, 128, 32, 32]               0\n",
            "           Conv2d-82          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-83          [-1, 128, 16, 16]             256\n",
            "             ReLU-84          [-1, 128, 16, 16]               0\n",
            "           Conv2d-85          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-86          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-87          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-88          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-89          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-90          [-1, 512, 16, 16]               0\n",
            "           Conv2d-91          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-92          [-1, 128, 16, 16]             256\n",
            "             ReLU-93          [-1, 128, 16, 16]               0\n",
            "           Conv2d-94          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-95          [-1, 128, 16, 16]             256\n",
            "             ReLU-96          [-1, 128, 16, 16]               0\n",
            "           Conv2d-97          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-98          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-99          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-100          [-1, 512, 16, 16]               0\n",
            "          Conv2d-101          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-102          [-1, 128, 16, 16]             256\n",
            "            ReLU-103          [-1, 128, 16, 16]               0\n",
            "          Conv2d-104          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-105          [-1, 128, 16, 16]             256\n",
            "            ReLU-106          [-1, 128, 16, 16]               0\n",
            "          Conv2d-107          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-109          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-110          [-1, 512, 16, 16]               0\n",
            "          Conv2d-111          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-112          [-1, 128, 16, 16]             256\n",
            "            ReLU-113          [-1, 128, 16, 16]               0\n",
            "          Conv2d-114          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
            "            ReLU-116          [-1, 128, 16, 16]               0\n",
            "          Conv2d-117          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-119          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-120          [-1, 512, 16, 16]               0\n",
            "          Conv2d-121          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-122          [-1, 128, 16, 16]             256\n",
            "            ReLU-123          [-1, 128, 16, 16]               0\n",
            "          Conv2d-124          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-125          [-1, 128, 16, 16]             256\n",
            "            ReLU-126          [-1, 128, 16, 16]               0\n",
            "          Conv2d-127          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-128          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-129          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-130          [-1, 512, 16, 16]               0\n",
            "          Conv2d-131          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-132          [-1, 128, 16, 16]             256\n",
            "            ReLU-133          [-1, 128, 16, 16]               0\n",
            "          Conv2d-134          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-135          [-1, 128, 16, 16]             256\n",
            "            ReLU-136          [-1, 128, 16, 16]               0\n",
            "          Conv2d-137          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-139          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-140          [-1, 512, 16, 16]               0\n",
            "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
            "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
            "            ReLU-143          [-1, 256, 16, 16]               0\n",
            "          Conv2d-144            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 8]             512\n",
            "            ReLU-146            [-1, 256, 8, 8]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
            "          Conv2d-149           [-1, 1024, 8, 8]         524,288\n",
            "     BatchNorm2d-150           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-151           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-152           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-153            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-154            [-1, 256, 8, 8]             512\n",
            "            ReLU-155            [-1, 256, 8, 8]               0\n",
            "          Conv2d-156            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-157            [-1, 256, 8, 8]             512\n",
            "            ReLU-158            [-1, 256, 8, 8]               0\n",
            "          Conv2d-159           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-160           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-161           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-162           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-163            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-164            [-1, 256, 8, 8]             512\n",
            "            ReLU-165            [-1, 256, 8, 8]               0\n",
            "          Conv2d-166            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-167            [-1, 256, 8, 8]             512\n",
            "            ReLU-168            [-1, 256, 8, 8]               0\n",
            "          Conv2d-169           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-170           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-171           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-172           [-1, 1024, 8, 8]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 1024, 1, 1]               0\n",
            "          Linear-174                    [-1, 3]           3,075\n",
            "================================================================\n",
            "Total params: 5,891,875\n",
            "Trainable params: 5,891,875\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 187.13\n",
            "Params size (MB): 22.48\n",
            "Estimated Total Size (MB): 209.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchvision import models\n",
        "#import torch\n",
        "#resnet50_pretrained = models.resnet50(pretrained=True)\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BSa9pgORrFPN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 layer의 filter를 확인해보자 (=가중치 확인) -> 나중에 학습을 완료한 후의 filter도 확인하기\n",
        "for w in resnet50.parameters():\n",
        "    w = w.data.cpu()\n",
        "    print(w.shape)\n",
        "    break\n",
        "\n",
        "# 가중치 renormalization\n",
        "min_w = torch.min(w)\n",
        "w1 = (-1/(2 * min_w)) * w + 0.5\n",
        "\n",
        "# make grid to display it\n",
        "grid_size = len(w1)\n",
        "x_grid = [w1[i] for i in range(grid_size)]\n",
        "x_grid = torchvision.utils.make_grid(x_grid, nrow=6, padding=1)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "imshow(x_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "tvmIA1dKrJSR",
        "outputId": "da63e5e5-a96f-4879-adb4-25a32f97f2f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 3, 3])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파라미터 설정 및 train 진행"
      ],
      "metadata": {
        "id": "z6RwHBy_rPRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config 모델 파라미터 인자를 만들기위한 클래스\n",
        "class Config:\n",
        "  def __init__(self, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "      setattr(self, key, value)"
      ],
      "metadata": {
        "id": "6HPTWGsirNjK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0008\n",
        "epochs = 30\n",
        "optimizer = 'Adam'"
      ],
      "metadata": {
        "id": "mUPc8BWArRhb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 클래스\n",
        "config = Config(\n",
        "    trainloader = trainloader,\n",
        "    testloader = testloader,\n",
        "    model = resnet50,\n",
        "    device = device,\n",
        "    optimizer = torch.optim.Adam(resnet50.parameters(), lr=lr),\n",
        "    criterion= nn.CrossEntropyLoss().to(device),\n",
        "    globaliter = 0\n",
        ")"
      ],
      "metadata": {
        "id": "9lBD5L_srSDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class train_test():\n",
        "      def __init__(self, config):\n",
        "        # 파라미터 인자\n",
        "        self.trainloader = config.trainloader\n",
        "        self.testloader = config.testloader\n",
        "        self.model = config.model\n",
        "        self.device = config.device\n",
        "        self.optimizer = config.optimizer\n",
        "        self.criterion = config.criterion\n",
        "        self.globaliter = config.globaliter\n",
        "        print(len(trainloader))\n",
        "      def train(self, epochs, log_interval):\n",
        "          self.model.train()\n",
        "          for epoch in range(1, epochs + 1 ):  # epochs 루프\n",
        "              running_loss = 0.0\n",
        "              lr_sche.step()\n",
        "              for i, data in enumerate(self.trainloader, 0): # batch 루프\n",
        "                  # get the inputs\n",
        "                  self.globaliter += 1\n",
        "                  inputs, labels = data # input data, label 분리\n",
        "                  inputs = inputs.to(self.device)\n",
        "                  labels = labels.to(self.device)\n",
        "\n",
        "                  # 가중치 초기화 -> 이전 batch에서 계산되었던 가중치를 0으로 만들고 최적화 진행\n",
        "                  self.optimizer.zero_grad() \n",
        "\n",
        "                  # forward + backward + optimize\n",
        "                  outputs = self.model(inputs)\n",
        "                  loss = self.criterion(outputs, labels)\n",
        "                  loss.backward()\n",
        "                  self.optimizer.step()\n",
        "                  running_loss += loss.item()\n",
        "\n",
        "                  # 30 iteration마다 acc & loss 출력\n",
        "                  if i % log_interval == log_interval -1 : # i는 1에포크의 iteration\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlearningLoss: {:.6f}\\twhole_loss: {:.6f} '.format(\n",
        "                        epoch, i*len(inputs), len(self.trainloader.dataset),\n",
        "                        100. * i*len(inputs) / len(self.trainloader.dataset), \n",
        "                        running_loss / log_interval,\n",
        "                        loss.item()))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "                    #with train_summary_writer.as_default():\n",
        "                    #    summary.scalar('loss', loss.item() , step = self.globaliter)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  self.model.eval()\n",
        "                  correct = 0\n",
        "                  total = 0\n",
        "                  test_loss = 0\n",
        "                  acc = []\n",
        "                  for k, data in enumerate(self.testloader, 0):\n",
        "                    images, labels = data\n",
        "                    images = images.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "                    outputs = self.model(images)\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                    test_loss += self.criterion(outputs, labels).item()\n",
        "                    acc.append(100 * correct/total)\n",
        "\n",
        "                  print('\\nTest set : Average loss:{:.4f}, Accuracy: {}/{}({:.0f}%)\\n'.format(\n",
        "                      test_loss, correct, total, 100 * correct/total\n",
        "                  ))\n",
        "                  #with test_summary_writer.as_default():\n",
        "                  #    summary.scalar('loss', test_loss , step = self.globaliter)\n",
        "                  #    summary.scalar('accuracy', 100 * correct/total , step = self.globaliter)  \n",
        "##                      if acc [k] > 60 and acc[k] > acc[k-1]:\n",
        "#                         torch.save({\n",
        "#                                     'epoch': epoch,\n",
        "#                                     'model_state_dict': self.model.state_dict(),\n",
        "#                                     'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "#                                     'loss': test_loss\n",
        "#                                     }, PATH)\n",
        "                         \n",
        "      print('Finished Training')"
      ],
      "metadata": {
        "id": "D1bPHz9VrU4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ready_to_train=train_test(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "iDUJKfgUrXGn",
        "outputId": "cbbbda15-9787-43b4-f649-a8d8491907fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3c587b6a4402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mready_to_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_sche = optim.lr_scheduler.StepLR(config.optimizer, step_size=10000, gamma=0.5) # 20 step마다 lr조정\n",
        "epochs = 200\n",
        "log_interval = 175\n",
        "\n",
        "ready_to_train.train(epochs, log_interval)"
      ],
      "metadata": {
        "id": "IjW_Fl7drXd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}